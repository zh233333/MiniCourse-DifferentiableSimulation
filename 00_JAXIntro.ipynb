{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMumXWmRYEPXSyMj0bqNxqc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidancrilly/MiniCourse-DifferentiableSimulation/blob/main/00_JAXIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An introduction to JAX:\n",
        "\n",
        "Resources:\n",
        "\n",
        "- JAX documentation: https://docs.jax.dev/en/latest/quickstart.html\n",
        "\n",
        "Below are the key libraries to import to use JAX, if you are familiar with regular numpy then jax.numpy will be easy to pick"
      ],
      "metadata": {
        "id": "qnRudVHjm429"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "GTofu7NXm4J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this introduction, we will go through the key capabilities of JAX, namely:\n",
        "\n",
        "- Just-in-time (JIT) compilation\n",
        "- Vectorising maps\n",
        "- Automatic differeniation\n",
        "\n",
        "These are implemented as JAX's key 'transformations':\n",
        "\n",
        "- jax.jit\n",
        "- jax.vmap\n",
        "- jax.grad\n",
        "\n",
        "These transformations take a function as an argument and return a transformed version of the function.\n",
        "\n",
        "The magic behind transformations is the notion of a JAX Tracer. Tracers are abstract stand-ins for array objects, and are passed to JAX functions in order to extract the sequence of operations that the function encodes. The need for these tracers to be abstract also restricts the kind of functions we can write as we shall see in the following.\n",
        "\n",
        "We won't aim to understand what exactly is happening at the low level but more review the functionality and how to use it for our use case."
      ],
      "metadata": {
        "id": "BnHe0XuKcfGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Just-in-time (JIT) compilation\n",
        "\n",
        "As the name suggests, JIT compiles a function at run time, just in time for its execution. JAX uses the Open XLA compiler ecosystem which natively supports accelerators.\n",
        "\n",
        "Lets try out jax.jit in a simple example."
      ],
      "metadata": {
        "id": "n_DEt7vdbpir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similatiries(x, y):\n",
        "  \"\"\"\n",
        "  Computes the cosine similarity between two vectors.\n",
        "\n",
        "  We will use very large input vectors for x and y\n",
        "  \"\"\"\n",
        "  x_norm = jnp.sum(x ** 2) ** 0.5\n",
        "  y_norm = jnp.sum(y ** 2) ** 0.5\n",
        "  return jnp.sum(x * y) / (x_norm * y_norm)\n",
        "\n",
        "def pythonic_cosine_similatiries(x, y):\n",
        "  \"\"\"\n",
        "  For comparison\n",
        "  \"\"\"\n",
        "  x = list(x)\n",
        "  y = list(y)\n",
        "  x_norm = 0.0\n",
        "  y_norm = 0.0\n",
        "  for i in range(len(x)):\n",
        "    x_norm += x[i] ** 2\n",
        "    y_norm += y[i] ** 2\n",
        "  x_norm = x_norm ** 0.5\n",
        "  y_norm = y_norm ** 0.5\n",
        "  return sum(x[i] * y[i] for i in range(len(x))) / (x_norm * y_norm)\n",
        "\n"
      ],
      "metadata": {
        "id": "c5PzxHRRbmAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To JIT compile code we simple call jax.jit on the function which returns the jitted function.\n",
        "\n",
        "**Note**: compiliation occurs whenever the shape of the inputs change or whenever a variable specified as static is changed in value."
      ],
      "metadata": {
        "id": "GFoLhdtHqeHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jitted_cosine_similatiries = jax.jit(cosine_similatiries)"
      ],
      "metadata": {
        "id": "eVovByxvqdk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test our functions, lets make some random numbers, we explicitly deal with the random number generator keys - a JAX quirk that you quickly get used to."
      ],
      "metadata": {
        "id": "RcIqEKtzd2Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10000\n",
        "\n",
        "seed = 1337\n",
        "key = jax.random.key(seed)\n",
        "\n",
        "key, subkey = jax.random.split(key)\n",
        "x = jax.random.normal(subkey, (N,))\n",
        "key, subkey = jax.random.split(key)\n",
        "y = jax.random.normal(subkey, (N,))"
      ],
      "metadata": {
        "id": "1WqnmvmidA98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare the run times of our various implementations, python, JAX with and without JIT (as well as see how long the code takes to compile)."
      ],
      "metadata": {
        "id": "_KGa2KJmd2lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "python_time = timeit.timeit(lambda: pythonic_cosine_similatiries(x, y), number=1)\n",
        "jax_time = timeit.timeit(lambda: cosine_similatiries(x, y), number=100)/100\n",
        "compile_time = timeit.timeit(lambda: jitted_cosine_similatiries(x, y), number=1)\n",
        "jit_time = timeit.timeit(lambda: jitted_cosine_similatiries(x, y), number=100)/100\n",
        "\n",
        "print(f'Python time: {1e3*python_time:.4f} ms')\n",
        "print(f'JAXPython time: {1e3*jax_time:.4f} ms')\n",
        "print(f'Compile time: {1e3*compile_time:.4f} ms')\n",
        "print(f'JIT time: {1e3*jit_time:.4f} ms')"
      ],
      "metadata": {
        "id": "YeKKwn3fdBzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JIT seems pretty amazing - it also can compile code for accelerators like GPU and CPU\n",
        "\n",
        "**However**, there are things you can't jit, including:\n",
        "\n",
        "- Functions with 'side effects' or impure functions, basically you need to pass all arguments to the function rather than relying on state\n",
        "- Functions which include dynamically shaped arrays, if any array in the function would change shape based on the inputs this makes the function not traceable\n",
        "- Functions which include python control flow (if/then/else), again this makes the function not traceable as you can't work out which branch of the if/then/else you should take at compile time.\n"
      ],
      "metadata": {
        "id": "ENkJUqjTd3ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variable_length_result(x,n):\n",
        "  res = jnp.linspace(0.0,x,n)\n",
        "  return res\n",
        "\n",
        "x = 0.25\n",
        "n = 5\n",
        "\n",
        "nojit_res = variable_length_result(x,n)\n",
        "print(nojit_res)"
      ],
      "metadata": {
        "id": "GxEq9AhKd6FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jitted_res = jax.jit(variable_length_result)(x,n)"
      ],
      "metadata": {
        "id": "U1PkGM_YkiGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# static_argnames tells JAX to recompile on changes at these argument positions:\n",
        "jitted_res = jax.jit(variable_length_result, static_argnames=['n'])(x,n)\n",
        "print(jitted_res)"
      ],
      "metadata": {
        "id": "n-JzOJz3mNV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above also applies for python-like if statements, in general these should be avoid and conditionals should be done with in-built JAX conditionals such as jnp.where"
      ],
      "metadata": {
        "id": "4M3ewfjmmeJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.linspace(0.0,1.0,100)\n",
        "y = jnp.where(x > 0.5, x, 0)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "u2Pde2ramrSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorising map (jax.vmap)\n",
        "\n",
        "Another key JAX functionality is the *vmap*, which allows you vectorise a function over (all or a subset) of its inputs. Basically, if you are performing the same operation along an axis of an array, vmap is made to handle this.\n",
        "\n",
        "Let us compute the mean and variance over the first axis of a 2D data array *x*, including a weighting term *w* which is the same for all rows of *x*."
      ],
      "metadata": {
        "id": "l3OHHRdgt-Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cumulants(x,w):\n",
        "  mu = jnp.sum(w*x)/jnp.sum(w)\n",
        "  var = jnp.sum(w*(x-mu)**2)/jnp.sum(w)\n",
        "  return mu, var\n",
        "\n",
        "# in_axes specifies over which axis the vectorising map should occur\n",
        "# None here says no vectorisation should be used, i.e. use the same w for all x\n",
        "vmapped_compute_cumulants = jax.vmap(compute_cumulants,in_axes=(1,None))\n",
        "\n",
        "Nx1 = 100\n",
        "Nx2 = 20\n",
        "\n",
        "key, subkey = jax.random.split(key)\n",
        "x = jax.random.normal(subkey, (Nx1,Nx2))\n",
        "key, subkey = jax.random.split(key)\n",
        "w = jax.random.normal(subkey, (Nx1))**2\n",
        "\n",
        "mu, var = vmapped_compute_cumulants(x,w)\n",
        "print(mu.shape)\n",
        "print(var.shape)"
      ],
      "metadata": {
        "id": "zSu4esJ9uj9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX quirks\n",
        "\n",
        "JAX's capabilities make for a few quirks when compared to native python/numpy code.\n",
        "\n",
        "## Random numbers\n",
        "\n",
        "We have already seen this in the random number generation at the top of this exercise!\n",
        "\n",
        "## In-place updates\n",
        "\n",
        "We must interact with JAX arrays in a different way than in python"
      ],
      "metadata": {
        "id": "63OXKkK_laZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.array([1.0,2.0,3.0,4.0])\n",
        "\n",
        "# This is not allowed\n",
        "try:\n",
        "  x[1] = -2.0\n",
        "except TypeError as e:\n",
        "  print('Numpy-like array updates not allowed:')\n",
        "  print(e)\n",
        "\n",
        "# You have to use the following syntax to perform in-place update\n",
        "x = x.at[1].set(-2.0)\n",
        "print('\\nUsing in-place updates:')\n",
        "print(x)"
      ],
      "metadata": {
        "id": "Zco1UOKYm1QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation\n",
        "\n",
        "## jax.grad\n",
        "\n",
        "jax.grad is the simpliest interface with JAX's automatic differentation (AD) capability. It works exclusively on scalar output functions (input can be multi-dimensional) - and it implements *reverse-mode* AD as this is most efficient for many-to-one functions."
      ],
      "metadata": {
        "id": "Z-obmONUbqCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sin_func(x):\n",
        "  return jnp.sin(x)\n",
        "\n",
        "x = jnp.linspace(-5,5,100)\n",
        "\n",
        "# Note we need to vmap our gradded function as jax.grad operates on scalar output functions only\n",
        "grad_sin_func = jax.vmap(jax.grad(sin_func))\n",
        "y = grad_sin_func(x)\n",
        "\n",
        "plt.title(r'No surprises here, $\\frac{d}{dx} \\sin (x) = \\cos (x)$')\n",
        "plt.plot(x,y,'r',lw=2)\n",
        "plt.plot(x,jnp.cos(x),'k--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fsIY-RNzboxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX also operates with what is known as \"Pytrees\", which are dictionaries of JAX arrays. This allows the naming of variables within a singular data structure which can make code easier to use as indexing is no longer required\n",
        "\n",
        "See the following example:"
      ],
      "metadata": {
        "id": "a6_iSLyWbhHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sin_variable_w_func(input):\n",
        "  return jnp.sin(input['w']*input['x'])\n",
        "\n",
        "# First for scalars\n",
        "input = {'w' : 2.0, 'x' : 1.0}\n",
        "\n",
        "scalar_grad_sin_variable_w_func = jax.grad(sin_variable_w_func)\n",
        "scalar_grad_sin_variable_w_func(input)"
      ],
      "metadata": {
        "id": "XhAziAr7bghc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now for scalar w and vector x\n",
        "\n",
        "input = {'w' : 2.0, 'x' : jnp.linspace(-5,5,100)}\n",
        "\n",
        "vector_x_grad_sin_variable_w_func = jax.vmap(scalar_grad_sin_variable_w_func,\n",
        "                                             in_axes=({'w' : None, 'x' : 0},))\n",
        "out = vector_x_grad_sin_variable_w_func(input)\n",
        "\n",
        "plt.title(r'No surprises here, $\\frac{d}{dw} \\sin (w x) = x \\cos (w x)$')\n",
        "plt.plot(x,out['w'],'r')\n",
        "plt.plot(x,input['x']*jnp.cos(input['w']*input['x']),'k--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ruSNILAXcS9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jax.jacfwd and jax.jacrev\n",
        "\n",
        "We can also compute the gradients of functions with multi-dimensional outputs, in particular we can compute the Jacobian of function $f(x)$ where $x$ is n-dimensional and $f(x)$ returns an m-dimensional vector.\n",
        "\n",
        "Mathematically, $J_f(x)$ is the Jacobian:\n",
        "\n",
        "$$\n",
        "  J_f(x) = \\begin{bmatrix} \\frac{d f_1}{dx_1} & \\dots & \\frac{d f_1}{dx_n} \\\\\n",
        "  \\vdots & & \\vdots \\\\\n",
        "  \\frac{d f_m}{dx_1} & \\dots & \\frac{d f_m}{dx_n} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can compute the Jacobian in forward- or reverse-mode automatic differentiation (jacfwd and jacrev respectively)."
      ],
      "metadata": {
        "id": "fMb6m-zSb1Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sin_cos_func(x):\n",
        "  \"\"\"\n",
        "  Test function with two outputs\n",
        "  \"\"\"\n",
        "  return jnp.array([jnp.sum(jnp.sin(x)), jnp.sum(jnp.cos(x))])\n",
        "\n",
        "x = jnp.linspace(-5,5,100)\n",
        "\n",
        "forwardmode_jac = jax.jacfwd(sin_cos_func)\n",
        "reversemode_jac = jax.jacrev(sin_cos_func)\n",
        "\n",
        "forwardmode_out = forwardmode_jac(x)\n",
        "reversemode_out = reversemode_jac(x)\n",
        "\n",
        "print(f'The shape of the forward-mode Jacobian is {forwardmode_out.shape}')\n",
        "print(f'The shape of the reverse-mode Jacobian is {reversemode_out.shape}')\n",
        "\n",
        "assert jnp.isclose(forwardmode_out,reversemode_out).all()\n",
        "# Note forwardmode_out =/= reversemode_out ! they involve different FLOPs"
      ],
      "metadata": {
        "id": "aHGUEU4Jb4ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We noted in the lecture that reverse-mode is more efficient for many-to-few functions, lets test this with our many-to-few test function"
      ],
      "metadata": {
        "id": "R1qmHfYkfQe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forwardmode_jac = jax.jit(forwardmode_jac)\n",
        "reversemode_jac = jax.jit(reversemode_jac)\n",
        "\n",
        "x_big = jnp.linspace(-5,5,1000)\n",
        "\n",
        "forwardmode_time = timeit.timeit(lambda: forwardmode_jac(x_big), number=100)/100\n",
        "reversemode_time = timeit.timeit(lambda: reversemode_jac(x_big), number=100)/100\n",
        "\n",
        "print(f'Forward-mode time: {1e3*forwardmode_time:.4f} ms')\n",
        "print(f'Reverse-mode time: {1e3*reversemode_time:.4f} ms')"
      ],
      "metadata": {
        "id": "GXc1HpC0fQs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Advanced**: Jacobian-vector products (JVPs) and vector-Jacobian products (VJPs)\n",
        "\n",
        "To really understand what is happening under the hood, we need to introduce JVPs and VJPs, which are related to forward- and reverse-mode AD respectively.\n",
        "\n",
        "Let’s say you have a function that takes several inputs and returns several outputs. Think of this like a physical system where you tweak some parameters (the inputs) and observe some measurements (the outputs). The Jacobian of this function is like a giant matrix of sensitivities—how each output changes with each input.\n",
        "\n",
        "But in practice, we don’t usually need the whole matrix when applying the chain rule.\n",
        "\n",
        "Instead, we want to know what happens in a particular direction. That’s where Jacobian-vector products (JVPs) and vector-Jacobian products (VJPs) come in.\n",
        "\n",
        "**Jacobian-Vector Product (JVP)**\n",
        "\n",
        "This is used in forward-mode AD. It answers the question:\n",
        "\n",
        "*“If I nudge my inputs a little bit in some direction, how do the outputs change?”*\n",
        "\n",
        "Imagine you’re computing a function, $f(x)$, and you have some small perturbation $\\delta x$. The JVP tells you how that perturbation propagates through the function:\n",
        "\n",
        "$$\n",
        "JVP =  J_f(x) \\cdot \\delta x\n",
        "$$\n",
        "\n",
        "You can think of the JVP like sending a small ripple through your system from the input side and seeing how it affects the output.\n",
        "\n",
        "The JVP is efficient when the function, $f(x)$, is few-to-many (small number of inputs, large number of outputs).\n"
      ],
      "metadata": {
        "id": "tmLUSusRb_kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_function(x):\n",
        "  res = {\n",
        "      'f0' : (jnp.sin(x[1])+x[2]*jnp.sin(x[3]))*jnp.exp(x[0]),\n",
        "      'f1' : (jnp.sin(x[1])+x[2]*jnp.sin(x[3]))*jnp.exp(-x[0])\n",
        "  }\n",
        "  return res\n",
        "\n",
        "x = jnp.array([1.0,2.0,3.0,4.0])\n",
        "\n",
        "# To get df/dx[i] for each i, we need to sweep through the JVP on the input space\n",
        "for i in range(len(x)):\n",
        "  v = jnp.eye(len(x))[i]\n",
        "  y, u = jax.jvp(test_function, (x,), (v,))\n",
        "  print(f'df/dx[{i}] = {u}')"
      ],
      "metadata": {
        "id": "wkKPE28D4Y6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector-Jacobian Product (VJP)**\n",
        "\n",
        "This is used in reverse-mode AD. It answers the question:\n",
        "\n",
        "*“How does a particular change in the outputs trace back to affect the inputs?”*\n",
        "\n",
        "$$\n",
        "VJP = \\delta y^T \\cdot J_f(x)\n",
        "$$\n",
        "\n",
        "This is like backpropagating a signal from the output side—asking how a change in a measurement pushes back through the system to influence the inputs. This is the basis for how gradients are computed efficiently when you have lots of inputs but only one output."
      ],
      "metadata": {
        "id": "8ooz0HBb4ZEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y, vjp_fun = jax.vjp(test_function, x)\n",
        "\n",
        "# To get df[j]/dx for each j, we need to sweep through the VJP on the output space\n",
        "v = {'f0' : 1.0, 'f1' : 0.0}\n",
        "u = vjp_fun(v)\n",
        "print(f'df[0]/dx = {u}')\n",
        "\n",
        "v = {'f0' : 0.0, 'f1' : 1.0}\n",
        "u = vjp_fun(v)\n",
        "print(f'df[1]/dx = {u}')"
      ],
      "metadata": {
        "id": "xG8MKWx6zF2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note we get the same answer as with the JVP but with two calls to the VJP vs four calls to the VJP.\n",
        "\n",
        "As a sanity check, here we compute the Jacobian using jacfwd and jacrev"
      ],
      "metadata": {
        "id": "Euu5lGDTqAd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.jacfwd(test_function)(x))\n",
        "print(jax.jacrev(test_function)(x))"
      ],
      "metadata": {
        "id": "5yRMSguMpAm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TdWqHQtVqMtL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}