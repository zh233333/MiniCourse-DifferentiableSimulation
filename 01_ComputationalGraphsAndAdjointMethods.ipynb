{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDZx9vBb33WhPcf1cyIVym",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidancrilly/MiniCourse-DifferentiableSimulation/blob/main/01_ComputationalGraphsAndAdjointMethods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "\n",
        "Resources:\n",
        "\n",
        "- JAX [documentation](https://jax.readthedocs.io/en/latest/quickstart.html)\n",
        "- B. Nikolic [blog post](https://bnikolic.co.uk/blog/python/jax/2022/02/22/jax-outputgraph-rev.html) on visualising jax graphs\n",
        "- Patrick Kidger \"On Neural Differential Equations\" [ArXiv link](https://arxiv.org/abs/2202.02435)"
      ],
      "metadata": {
        "id": "3sou0T-JMYHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import jax"
      ],
      "metadata": {
        "id": "B9VEb6hyMYng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, we will look at computational graphs and automatic differentiaton. For this we will use Python JAX, a machine learning framework developed by Google.\n",
        "\n",
        "To aid our understanding, we will visualise the computational graphs using the functions below. It is not important to understand exactly how this is done."
      ],
      "metadata": {
        "id": "lo3R9RSs_iGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pydot\n",
        "from jaxlib import xla_client\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def view_pydot(pdot):\n",
        "  plt = Image(pdot.create_png())\n",
        "  display(plt)\n",
        "\n",
        "def todotgraph(x):\n",
        "  \"\"\"\n",
        "  Credit to: https://bnikolic.co.uk/blog/python/jax/2022/02/22/jax-outputgraph-rev.html\n",
        "  \"\"\"\n",
        "  return xla_client._xla.hlo_module_to_dot_graph(xla_client._xla.hlo_module_from_text(x))\n",
        "\n",
        "def viz_computational_graph(f,xs,compiled=False,static_argnums=None):\n",
        "  \"\"\"\n",
        "  Produces a visualisation of a computational graph for function f with input x\n",
        "  \"\"\"\n",
        "  if(compiled):\n",
        "    z = jax.jit(f,static_argnums=static_argnums).lower(*xs).compile().as_text()\n",
        "    z = todotgraph(z)\n",
        "  else:\n",
        "    z = jax.jit(f,static_argnums=static_argnums).lower(*xs).compiler_ir('hlo').as_hlo_dot_graph()\n",
        "  with open(\"t.dot\", \"w\") as f:\n",
        "    f.write(z)\n",
        "  (graph,) = pydot.graph_from_dot_file('t.dot')\n",
        "  view_pydot(graph)"
      ],
      "metadata": {
        "id": "JhkvfYDy_jBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1\n",
        "\n",
        "We will write a number of simple functions, use JAX's AD to differentiate them and visualise the computational graphs.\n",
        "\n",
        "For this part of the exercise, we define 4 functions of interest:\n",
        "\n",
        "tanh(x) : $$ \\tanh(\\vec{x}) = \\frac{1-e^{-2\\vec{x}}}{1+e^{-2\\vec{x}}} $$\n",
        "\n",
        "grad_tanh(x) : $$ \\frac{d\\tanh(x)}{dx} = 1 - \\tanh ^2 (x) $$\n",
        "\n",
        "log_mean_tanh(x) : $$ \\text{log_mean_tanh}(\\vec{x}) = \\ln\\left(\\frac{1}{N}âˆ‘_i^N \\tanh(x_i) \\right)$$\n",
        "\n",
        "grad_log_mean_tanh(x) : $$ \\text{grad_log_mean_tanh}(\\vec{x}) = \\frac{\\partial \\text{log_mean_tanh}}{\\partial \\vec{x}}  $$\n",
        "\n",
        "Below are a number of incomplete function defintions. Fill in the missing parts, making use of the JAX numpy library, and use viz_computational_graph to visualise their graphs.\n",
        "\n",
        "N.B. To create the graph we need to \"stage out\" a specialised version of the Python function to one that operates on restricted data types (see https://jax.readthedocs.io/en/latest/aot.html for detail). To do this we must provide an argument to the function, in the following we will simple use a 1-D array of ones with length 100.\n",
        "\n",
        "Some tips for reading the graphs:\n",
        "\n",
        "1. Data types are explicitly given, any static operands will typically show up as f32[].\n",
        "2. For binary operations, the numbers on top of the arrows into the graph node denote the position of the operand. For example, into a subtract operation the result is {Operand 0} - {Operand 1}.\n",
        "3. Without compilation, the computational graphs are no necessarily optimised. When compiled, optimisations such as loop fusion will be included. While optimal, they can be less human-interpretable.\n",
        "\n",
        "Visualise and inspect the results for each function. Some questions to ask yourself: do they make sense? For gradient operations, is there a clear extension of the original graph? How does compilation alter the graph?\n",
        "\n",
        "First, lets start with tanh(x) and grad_tanh(x):"
      ],
      "metadata": {
        "id": "wlwnPtvn-yEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "  \"\"\"\n",
        "  To be completed\n",
        "  \"\"\"\n",
        "\n",
        "  return res\n",
        "\n",
        "def dtanh(x):\n",
        "  \"\"\"\n",
        "  Explicit analytic derivative\n",
        "  \"\"\"\n",
        "  res = 1-(tanh(x))**2\n",
        "  return res\n",
        "\n",
        "def grad_tanh(x):\n",
        "  \"\"\"\n",
        "  Automatic Derivative of user defined tanh function\n",
        "\n",
        "  N.B. jax.grad operates on scalar output functions\n",
        "  we therefore need to add a jax.vmap to compute the\n",
        "  derivative of every element in input x\n",
        "  \"\"\"\n",
        "  res = jax.vmap(jax.grad(tanh))(x)\n",
        "  return res\n",
        "\n",
        "# Try visualising the computational graphs!\n",
        "viz_computational_graph(dtanh,[jnp.ones(100)],compiled=False)\n"
      ],
      "metadata": {
        "id": "9DPbsovH8evP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check (graphically) that your analytic and automatic derivatives of tanh function agree."
      ],
      "metadata": {
        "id": "bs0542mOKG4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.linspace(-1.0,1.0,100)\n",
        "\n",
        "fig = plt.figure(dpi=200,figsize=(4,2))\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax1.plot(x,dtanh(x))\n",
        "ax1.plot(x,grad_tanh(x),ls='--')\n",
        "\n",
        "ax2.plot(x,dtanh(x)-grad_tanh(x),c='k')\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "YpWytmPkJPHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us at look at more complex functions which take many inputs but produce a scalar output?"
      ],
      "metadata": {
        "id": "Sjt0co0Xu9iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mean_tanh(x):\n",
        "  \"\"\"\n",
        "  To be completed\n",
        "  \"\"\"\n",
        "\n",
        "  return res\n",
        "\n",
        "def grad_log_mean_tanh(x):\n",
        "  \"\"\"\n",
        "  To be completed\n",
        "  \"\"\"\n",
        "\n",
        "  return res\n",
        "\n",
        "viz_computational_graph(log_mean_tanh,[jnp.ones(100)],compiled=False)"
      ],
      "metadata": {
        "id": "Xb7gLiPCJFvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "1.   True or False: All the nodes in the graphs constructed above are for binary operations - i.e. have two arrows into them.\n",
        "2.   The computational graphs of the autograd of tanh and an explicit analytic gradient of tanh are identical?\n",
        "3. Why is the jax.vmap needed in jax.vmap(jax.grad(tanh)) but not for grad_log_mean_tanh?\n",
        "4. What is the reason why the autograd and analytic gradients of tanh disagree at the 1e-7 level?\n",
        "5. How are constant floats included in the graphs? What does this tell you about how the functions are compiled (hint: do you have assume something about the inputs)?"
      ],
      "metadata": {
        "id": "IYLZxV_txmA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2a\n",
        "\n",
        "We will now use the above techniques to inspect how AD computes the adjoint solution of an ODE. We will follow the example from the lecture notes, an exponential decay:\n",
        "\n",
        "$$ \\frac{dy}{dt} = - \\frac{y}{\\tau} $$\n",
        "\n",
        "The simplest numerical method to solve this ODE is the Euler method. This uses the first order Taylor expansion to integrate forward in time:\n",
        "\n",
        "$$ y^{i+1} = y^{i} - \\frac{dt}{\\tau} y^{i} $$\n",
        "\n",
        "In the following exercise we will construct an Euler integrator for this ODE and use JAX AD to compute gradients through this numerical solution. We will compare the results to the analytic solutions to this problem.\n",
        "\n",
        "In the following code block, you will write functions which define the exponential decay ODE and its analytic forward and adjoint solutions."
      ],
      "metadata": {
        "id": "uueqU6CGEdTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dydt(t,y,args):\n",
        "  \"\"\"\n",
        "  To be completed\n",
        "\n",
        "  note args is a dict with key 'tau'\n",
        "\n",
        "  \"\"\"\n",
        "  return res\n",
        "\n",
        "def analytic_solution(y0,T,tau):\n",
        "  \"\"\"\n",
        "  To be completed\n",
        "\n",
        "  Analytic solution of exponential decay ODE\n",
        "  at time t = T, with initial condition y(0) = y0\n",
        "\n",
        "  \"\"\"\n",
        "  return res\n",
        "\n",
        "def adjoint_analytic_solution(y0,y1,T,tau,loss_fn):\n",
        "  dLdyT = jax.grad(loss_fn)(y1)\n",
        "  return T/tau**2*dLdyT*analytic_solution(y0,T,tau)"
      ],
      "metadata": {
        "id": "Ux3xl-uf9vD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code block, you will construct the Euler numerical integrator for this problem."
      ],
      "metadata": {
        "id": "biW68hudIOFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass_explicit_Euler(y0,t0,t1,Nt,tau,loss_fn):\n",
        "  dt = (t1-t0)/(Nt)\n",
        "  args = {'tau' : tau}\n",
        "\n",
        "  def _integrate(i,yi):\n",
        "    \"\"\"\n",
        "    Single time step using explicit Euler differencing\n",
        "\n",
        "    y_i+1 = y_i + dydt(t_i,y_i)*dt\n",
        "\n",
        "    To be completed\n",
        "    \"\"\"\n",
        "\n",
        "    return yip1\n",
        "\n",
        "  def _forward_pass():\n",
        "    \"\"\"\n",
        "    Integrate ODE forward using for loop\n",
        "    \"\"\"\n",
        "    y1 = jax.lax.fori_loop(0,Nt,_integrate,y0)\n",
        "    L = loss_fn(y1)\n",
        "    return L,y1\n",
        "\n",
        "  return _forward_pass()"
      ],
      "metadata": {
        "id": "dNrewHm_Emyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you are happy with your Euler integrator, run the following cell to compare your numerical solution with the analytic solution."
      ],
      "metadata": {
        "id": "ZRfXSILrNGKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use very simple 'loss' function in this example\n",
        "def identity(x):\n",
        "  return x\n",
        "\n",
        "tau = 0.5\n",
        "\n",
        "t0 = 0.0\n",
        "t1 = 1.0\n",
        "Nt = 100\n",
        "\n",
        "y0 = 1.0\n",
        "\n",
        "L,y1 = forward_pass_explicit_Euler(y0,t0,t1,Nt,tau,identity)\n",
        "y1_analytic = analytic_solution(y0,t1-t0,tau)\n",
        "print(f'Numerical solution: {y1}, and analytic solution: {y1_analytic}')"
      ],
      "metadata": {
        "id": "480XBA223V-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualise our ODE solver graph,\n",
        "\n",
        "We note a couple of important features:\n",
        "1. The ODE integration takes the form of a loop as expected. We have the main function which sets up the input to the loop. The internal loop which increments our $ y $ value and a counter. Finally, we have the single Euler step calculation.\n",
        "2. The single Euler step is included as a subcomputation of the loop. It combines $ \\tau $ and the previous loop value. As the time step has been defined in a static manner, it is included as a floating point constant in the multiply"
      ],
      "metadata": {
        "id": "F9F0aYPFFARx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One can use a lambda function to make some arguments treated as 'static'\n",
        "tau_fn = lambda x : forward_pass_explicit_Euler(y0,t0,t1,Nt,x,identity)[0]\n",
        "viz_computational_graph(tau_fn,[tau],compiled=False)\n",
        "\n",
        "# Or alternatively, one can specify with static_argnums\n",
        "# viz_computational_graph(forward_pass_explicit_Euler,(y0,t0,t1,Nt,tau,identity),compiled=False,static_argnums=[3,5])"
      ],
      "metadata": {
        "id": "FYar-gMZAMBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use JAX's AD capabilities to compute the adjoint, $dL/d\\tau$, based on the Euler integrator function. This is as simple as using jax.grad!\n",
        "\n",
        "We can compare this numerical AD solution to the analytic result:"
      ],
      "metadata": {
        "id": "NNRiXPutmzgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grad_tau_fn = jax.grad(tau_fn)\n",
        "tau_adjoint_val = adjoint_analytic_solution(y0,y1,t1-t0,tau,identity)\n",
        "\n",
        "print(f'Numerical adjoint solution: {grad_tau_fn(tau)}, and analytic adjoint solution: {tau_adjoint_val}')"
      ],
      "metadata": {
        "id": "2CRuZKVq570h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The computational graph for the AD adjoint will be complex, therefore I would not recommend spending too much time looking over its details. It might be informative to identify the following:\n",
        "\n",
        "1. The number of loops used\n",
        "2. Where the Euler forward step appears\n",
        "3. Whether the foward or backward solve involves more operations per loop"
      ],
      "metadata": {
        "id": "XqFoSkiSIlkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "viz_computational_graph(grad_tau_fn,[tau],compiled=False)"
      ],
      "metadata": {
        "id": "Gush1amj7Z--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2b\n",
        "\n",
        "Here we will use AD to construct the adjoint equations themselves and then solve them numerically. We can then contrast these results with the AD derivative of the forward solution. In other words we will compare the 'optimise-then-discretise' and 'discretise-then-optimise' strategies.\n",
        "\n",
        "Following Kidger's thesis, we form two adjoint equations using derivatives of the forward model. For a forward model of the form:\n",
        "\n",
        "$$ \\frac{dy}{dt} = f(t,y,\\theta) $$\n",
        "\n",
        "Where $\\theta$ are our model parameters. In our case of the exponential decay there is only one parameter, $\\tau$.\n",
        "\n",
        "Given a loss function, $L$, that is a function of only the terminal value, $y(T)$, we have the following adjoint equations:\n",
        "\n",
        "$$ \\frac{d a_y}{dt} = - a_y \\frac{\\partial f}{\\partial y} $$\n",
        "$$ \\frac{d a_\\theta}{dt} = - a_y \\frac{\\partial f}{\\partial \\theta} $$\n",
        "\n",
        "with the following initial conditions:\n",
        "\n",
        "$$ a_y(T) = \\frac{dL}{dy(T)} \\ , \\ a_\\theta(T) = 0 $$\n",
        "\n",
        "To find the gradients we need we must therefore solve the adjoint equations backwards in time. In this exercise we will use Euler's method for simplicity. The numerical system therefore looks like:\n",
        "\n",
        "$$ [a_y^{j+1} \\ , \\ a_{\\theta}^{j+1}] = \\left[a_y^{j} \\ , \\ a_{\\theta}^{j} \\right] - dt \\left[ -a_y^{j} \\ \\frac{\\partial f}{\\partial y}(t^{j},y^{j},\\theta) \\ , \\ -a_y^{j} \\  \\frac{\\partial f}{\\partial \\theta}(t^{j},y^{j},\\theta) \\right] $$\n",
        "\n",
        "N.B. the index $j$ increases as we step backwards in time.\n",
        "\n",
        "The following code is set up to solve this problem but there are a few functions to be completed"
      ],
      "metadata": {
        "id": "WOSGo6DBIihn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_cond(loss_fn,yT):\n",
        "  \"\"\"\n",
        "  Gives the initial conditions for the adjoint equations\n",
        "  \"\"\"\n",
        "  dLdyT = jax.grad(loss_fn)(yT)\n",
        "  return jnp.array([dLdyT,0.0])\n",
        "\n",
        "def adjoint_forcing_term(t,a,args):\n",
        "  \"\"\"\n",
        "  To be completed\n",
        "\n",
        "  Compute dadt, the RHS of the adjoint equations\n",
        "  \"\"\"\n",
        "  dfdy = jax.grad(dydt,argnums=1)\n",
        "  dfdtheta = jax.grad(dydt,argnums=2)\n",
        "  y = get_y_value(t)\n",
        "  return dadt\n",
        "\n",
        "def get_y_value(t):\n",
        "  \"\"\"\n",
        "  A function to get the y value from the forward solution at time t\n",
        "\n",
        "  Normally this would be obtained by an interpolation on the forward numerical solution\n",
        "\n",
        "  We will slightly cheat here for the sake of simplicity and use the analytic result\n",
        "  \"\"\"\n",
        "  return analytic_solution(y0,t,tau)"
      ],
      "metadata": {
        "id": "rkRV0altDjip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An below we have a numerical integrator for the adjoint equations, again note that index j steps backwards in time."
      ],
      "metadata": {
        "id": "-B2uhbGmpBD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass_explicit_Euler(y1,t0,t1,Nt,tau,loss_fn):\n",
        "  dt = (t1-t0)/(Nt)\n",
        "  args = {'tau' : tau}\n",
        "  a1 = init_cond(loss_fn,y1)\n",
        "  # s array is (a,t)\n",
        "  s1 = jnp.append(a1,t1)\n",
        "\n",
        "  def _integrate(j,sj):\n",
        "    \"\"\"\n",
        "    Single time step using explicit Euler differencing\n",
        "\n",
        "    To be completed, include equations for next adjoint values and time step\n",
        "\n",
        "    \"\"\"\n",
        "    aj = sj[:2]\n",
        "    tj = sj[2]\n",
        "    ajp1 =\n",
        "    tjp1 =\n",
        "    sjp1 = jnp.append(ajp1,tjp1)\n",
        "    return sjp1\n",
        "\n",
        "  def _backward_pass():\n",
        "    \"\"\"\n",
        "    Integrate ODE using for loop\n",
        "    \"\"\"\n",
        "    s0 = jax.lax.fori_loop(0,Nt,_integrate,s1)\n",
        "    return s0[:2],s0[2]\n",
        "\n",
        "  return _backward_pass()"
      ],
      "metadata": {
        "id": "vapGaSGbUSyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our backward pass Euler integrator complete, try the cell below to compute the adjoint solution using the optimise-then-discretise strategy."
      ],
      "metadata": {
        "id": "8fFfhkqypBjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N.B. Nt_adjoint does not need to equal the Nt used for the forward solution for 'optimise-then-discretise' approach\n",
        "Nt_adjoint = 100\n",
        "\n",
        "a0,t_end = backward_pass_explicit_Euler(y1,t0,t1,Nt_adjoint,tau,identity)"
      ],
      "metadata": {
        "id": "rC96YMDwa_w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare the accuracy of the two numerical approaches to computing an adjoint against the analytic solution."
      ],
      "metadata": {
        "id": "6P_2c-w4onYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Numerical \"discretise-then-optimise\" adjoint: {grad_tau_fn(tau)}, \\nNumerical \"optimise-then-discretise\" adjoint: {a0[1]}, \\nand analytic adjoint solution: {tau_adjoint_val}')"
      ],
      "metadata": {
        "id": "i5wdTRQebICV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can compare the convergence behaviour of the two adjoint solution methods with respect to the number of Euler steps used in the integrators."
      ],
      "metadata": {
        "id": "7ZY1hITEgXNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Nts = [10,100,1000,10000]\n",
        "\n",
        "fig = plt.figure(dpi=200,figsize=(4,3))\n",
        "ax1 = fig.add_subplot(111)\n",
        "\n",
        "o2d_adjoint_fn = lambda x,N : backward_pass_explicit_Euler(y1,t0,t1,N,x,identity)[0][1]\n",
        "d2o_adjoint_fn = lambda x,N : jax.grad(forward_pass_explicit_Euler,argnums=4,has_aux=True)(y0,t0,t1,N,x,identity)[0]\n",
        "\n",
        "for Nt in Nts:\n",
        "  ax1.plot(Nt,o2d_adjoint_fn(tau,Nt),'bo')\n",
        "  ax1.plot(Nt,d2o_adjoint_fn(tau,Nt),'ro')\n",
        "\n",
        "ax1.axhline(tau_adjoint_val,c='k')\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_ylabel(r'$\\frac{dL}{d\\tau}$')\n",
        "ax1.set_xlabel(\"# Euler steps\")"
      ],
      "metadata": {
        "id": "P5aT35qNb9tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "1.   True or False: The discretise-then-optimise and optimise-then-discretise strategies for computing the adjoint involve a different number of loops.\n",
        "2.   Which of the two strategies involves the AD propagating through a graph with a loop?\n",
        "3.   Which of the two stategies would most easily allow a different time stepping scheme for the forward and adjoint computations?"
      ],
      "metadata": {
        "id": "mG6jTalcyvey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise completed!\n",
        "\n",
        "Key takeaways:\n",
        "\n",
        "- Automatic Differentiation (AD) allows accurate computation of gradients of complex numerical functions. AD makes use of computational graphs to achieve this.\n",
        "- The numerical solutions to differential equations are no different than other complex numerical functions. AD can be used to two ways to backpropagate gradients through differential equations:\n",
        "  1. 'Discretise-then-optimise': One discretises the differential equation and writes a numerical integrator. AD traces through the graph of this numerical integrator to produce an adjoint solution.\n",
        "  2. 'Optimise-then-discretise': One dicretises both the differential equation *and* its adjoint equations. AD can be used to compute the derivative terms that appear in the adjoint equations' forcing terms.\n",
        "- AD therefore gives us a clear path towards differentiable simulators!"
      ],
      "metadata": {
        "id": "iOKV69xXp0i1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgoTEo01hGdU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}