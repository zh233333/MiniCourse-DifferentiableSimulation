{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zh233333/MiniCourse-DifferentiableSimulation/blob/main/02_DifferentiableSimulatorsAndOptimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "\n",
        "Resources:\n",
        "\n",
        "- JAX [documentation](https://jax.readthedocs.io/en/latest/quickstart.html)\n",
        "- Patrick Kidger \"On Neural Differential Equations\" [ArXiv link](https://arxiv.org/abs/2202.02435)\n",
        "\n",
        "Note: need to install diffrax and optax libraries as not installed by default on colab"
      ],
      "metadata": {
        "id": "Ol9R_T9VkAcs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um1XqNf7j3IM"
      },
      "outputs": [],
      "source": [
        "!pip install diffrax optax\n",
        "import diffrax\n",
        "import optax\n",
        "import matplotlib.pyplot as plt\n",
        "import jax.numpy as jnp\n",
        "import jax"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1\n",
        "\n",
        "For the following exercises we are going to introduce two JAX libraries which will allow us to easily write and train differentiable simulators:\n",
        "\n",
        "1. diffrax - https://docs.kidger.site/diffrax/\n",
        "2. optax - https://optax.readthedocs.io/en/latest/getting_started.html\n",
        "\n",
        "These libraries allow for the numerical forward and adjoint solution to differential equations (diffrax) and the solving of optimisation problems (optax).\n",
        "\n",
        "First, we introduce diffrax. The key features of diffrax we need to use are:\n",
        "\n",
        "1. ODETerms, these wrap python functions of the form:\n",
        "```\n",
        "def dydt(t : float, y : JAX array, args : dict):\n",
        "    return dydt_val : JAX array\n",
        "```\n",
        "which return the right hand side of your differential equation.\n",
        "\n",
        "2. Solvers, these implement different numerical methods for evolving in time. For example, Euler's method is available as well as other higher order methods.\n",
        "\n",
        "3. Step size controllers, the solvers we will use are adaptive (they vary the time step used) so these controllers are used to decide what step size to take to obtain a prescribed accuracy\n",
        "\n",
        "4. SaveAts, these simply provide a number of points in time when the numerical solution should be saved and returned to the user.\n",
        "\n",
        "Using these features, diffrax numerically solves the following equation:\n",
        "\n",
        "$$\n",
        "y(t_1) = y(t_0) + \\int_{t_0}^{t_1} \\frac{dy}{dt} dt\n",
        "$$\n",
        "\n",
        "and thus we must also specify the time interval $[t_0,t_1]$ and the intitial conditions $y(t_0) \\equiv y_0$.\n",
        "\n",
        "## Part a\n",
        "\n",
        "We will first learn to use the key features of diffrax using the exponential decay ODE example from the previous exercise.\n"
      ],
      "metadata": {
        "id": "r8VpdkwHkVAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dydt(t,y,args):\n",
        "  return -y/args['tau']\n",
        "\n",
        "def diffrax_solve(dydt,t0,t1,Nt,rtol=1e-5,atol=1e-5):\n",
        "  \"\"\"\n",
        "  Here we wrap the diffrax diffeqsolve function such that we can run with\n",
        "  different y0s and taus over the same time interval easily\n",
        "  \"\"\"\n",
        "  # We convert our python function to a diffrax ODETerm\n",
        "  term = diffrax.ODETerm(dydt)\n",
        "  # We chose a solver (time-stepping) method from within diffrax library\n",
        "  # Heun's method (https://en.wikipedia.org/wiki/Heun%27s_method)\n",
        "  solver = diffrax.Heun()\n",
        "\n",
        "  # At what time points you want to save the solution\n",
        "  saveat = diffrax.SaveAt(ts=jnp.linspace(t0,t1,Nt))\n",
        "  # Diffrax uses adaptive time stepping to gain accuracy within certain tolerances\n",
        "  stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
        "\n",
        "  return lambda y0,tau : diffrax.diffeqsolve(term, solver,\n",
        "                         y0=y0, args = {'tau' : tau},\n",
        "                         t0=t0, t1=t1, dt0=(t1-t0)/Nt,\n",
        "                         saveat=saveat, stepsize_controller=stepsize_controller)\n",
        "\n",
        "t0 = 0.0\n",
        "t1 = 1.0\n",
        "Nt = 100\n",
        "\n",
        "ODE_solve = diffrax_solve(dydt,t0,t1,Nt)\n",
        "\n",
        "# Solve for specific y0 and tau\n",
        "y0 = 1.0\n",
        "tau = 0.5\n",
        "sol = ODE_solve(y0,tau)\n",
        "\n",
        "plt.plot(sol.ts,sol.ys)\n",
        "plt.plot(sol.ts,y0*jnp.exp(-sol.ts/tau),'k--')\n",
        "plt.legend(['numerical','exact'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1LgCzFOfj7PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diffrax solutions are differentiable by construction - see https://docs.kidger.site/diffrax/api/adjoints/ for details.\n",
        "\n",
        "We can therefore very easily solve the adjoint state problem using diffrax and JAX AD:"
      ],
      "metadata": {
        "id": "DkVeJhuBUNcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(inputs):\n",
        "  y0 = inputs['y0']\n",
        "  tau = inputs['tau']\n",
        "  sol = ODE_solve(y0,tau)\n",
        "  return sol.ys[-1]\n",
        "\n",
        "inputs = {'y0' : y0, 'tau' : tau}\n",
        "# Returns gradient of loss with respect to all inputs, i.e. dLdtau and dLdy0\n",
        "jax.grad(loss)(inputs)"
      ],
      "metadata": {
        "id": "4vvRNDxjUNzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part b\n",
        "\n",
        "So far we have only considered ordinary differential equations, the only derviatives that appeared were total derivates w.r.t. time. However, Partial Differential Equations (PDEs) are key to physics modelling and involve partial derivatives w.r.t. multiple variables.\n",
        "\n",
        "To solve PDEs with diffrax, we will use the method of lines (https://en.wikipedia.org/wiki/Method_of_lines), leaving time as a continuous variable. Therefore we can write our dydt function needed for the diffrax ODETerm and allow diffrax to handle the time differencing.\n",
        "\n",
        "In the following exercise we are concerned with the following transport equation over the ($t,x,\\mu$) domain:\n",
        "\n",
        "$$ \\frac{\\partial f}{\\partial t} = -\\mu \\frac{\\partial f}{\\partial x} + \\sigma \\left(\\frac{1}{2}\\phi - f \\right)+ \\frac{1}{2} S(x,t) $$\n",
        "$$ \\phi(x,t) = \\int f(x,\\mu,t) d\\mu $$\n",
        "\n",
        "Physically, this equation represents the transport of an ensemble of scattering particles in one spatial dimension, $x$. The direction domain, $\\mu \\equiv \\cos(\\theta)$, is bounded to between $\\pm 1$. The scattering rate, $\\sigma$, is a single scalar which dictates the dynamics of the system.\n",
        "\n",
        "Applying finite differening (https://en.wikipedia.org/wiki/Finite_difference_method) and the method of lines to the above equation gives us the following numerical equations:\n",
        "\n",
        "$$\n",
        "\\mu_m < 0 \\ , \\ \\frac{\\partial f_{i,m}}{\\partial t} = -\\mu_m \\frac{f_{i+1,m}-f_{i,m}}{\\Delta x} + \\sigma \\left(\\frac{1}{2}\\phi_i - f_{i,m} \\right)+ \\frac{1}{2} S(x_i,t) \\\\\n",
        "\\mu_m > 0 \\ , \\ \\frac{\\partial f_{i,m}}{\\partial t} = -\\mu_m \\frac{f_{i,m}-f_{i-1,m}}{\\Delta x} + \\sigma \\left(\\frac{1}{2}\\phi_i - f_{i,m} \\right)+ \\frac{1}{2} S(x_i,t) \\\\\n",
        "\\phi_i = \\sum_m w_m f_{i,m}\n",
        "$$\n",
        "Where we have upwinded (https://en.wikipedia.org/wiki/Upwind_scheme) the first order spatial differencing - this makes the numerical solution conditionally stable.\n",
        "\n",
        "Complete the below function to implement this finite differencing:"
      ],
      "metadata": {
        "id": "-eLRydYUZG7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_finite_difference(f,mu_m,w_m,sig,dx):\n",
        "  Nm,Nx = f.shape\n",
        "  dfdt_advection = jnp.zeros((Nm,Nx))\n",
        "  dfdt_scattering = jnp.zeros((Nm,Nx))\n",
        "  # Computing the finite difference equations as above\n",
        "  # Tactics:\n",
        "  # 1. mu_m > 0 resides at indices [Nm//2:,:]\n",
        "  #\n",
        "  # 2. Adding 'ghost' cells is a common method to handle boundary conditions\n",
        "  # Here the boundary conditions are that f is zero outside x-domain\n",
        "  #\n",
        "  # This can be achieved like so:\n",
        "  # f_advection_ghost = jnp.concatenate([jnp.zeros((Nm,1)),f,jnp.zeros((Nm,1))],axis=1)\n",
        "  #\n",
        "  # This allows slices of the array to be used to compute finite differences\n",
        "  #\n",
        "  # 3. You will need to think about the broadcast of arrays as mu_m and w_m are shape (Nm)\n",
        "  # and the solution array is shape (Nm,Nx)\n",
        "  #\n",
        "  f_advection_ghost = jnp.concatenate([jnp.zeros((Nm,1)),f,jnp.zeros((Nm,1))],axis=1)\n",
        "  dfdt_advection = dfdt_advection.at[:Nm//2,:].set(???)\n",
        "  dfdt_advection = dfdt_advection.at[Nm//2:,:].set(???)\n",
        "  dfdt_advection = -mu_m[:,None]*dfdt_advection/dx\n",
        "\n",
        "  scattering_sink   =\n",
        "  scattering_source =\n",
        "  dfdt_scattering = scattering_sink+scattering_source[None,:]\n",
        "\n",
        "  return dfdt_advection+dfdt_scattering"
      ],
      "metadata": {
        "id": "9MhXEFUYafng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now wrap the finite difference equations into a dydt function compatiable with diffrax:\n",
        "\n",
        "Note you must complete this function by adding the source term"
      ],
      "metadata": {
        "id": "HaE6i8FIIMMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transport_equation_dydt(t,y,args):\n",
        "  Nm,Nx = args['Nm'],args['Nx']\n",
        "  f = y.reshape((Nm,Nx))\n",
        "  # Compute finite differences\n",
        "  dfdt = compute_finite_difference(f,args['mu_m'],args['w_m'],args['sig'],args['dx'])\n",
        "  # Add isotropic source term\n",
        "  # This is stored in args['S'] and is a function of t only\n",
        "  dfdt += 0.5*args['S'](t)[None,:]\n",
        "  dydt = dfdt.reshape(-1)\n",
        "  return dydt"
      ],
      "metadata": {
        "id": "XdXaS5vUhrsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in the case for the exponential decay ODE, we define a number of diffrax objects and the time interval needed to numerical solve our problem:"
      ],
      "metadata": {
        "id": "6aLQetofIqPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We convert our python function to a diffrax ODETerm\n",
        "transport_equation_term = diffrax.ODETerm(transport_equation_dydt)\n",
        "# We chose a solver (time-stepping) method from within diffrax library\n",
        "# Heun's method (https://en.wikipedia.org/wiki/Heun%27s_method)\n",
        "solver = diffrax.Heun()\n",
        "\n",
        "# Diffrax uses adaptive time stepping to gain accuracy within certain tolerances\n",
        "stepsize_controller = diffrax.PIDController(rtol=1e-3, atol=1e-5)\n",
        "\n",
        "t0 = 0.0\n",
        "t1 = 10.0\n",
        "Nt = 10\n",
        "dt0 = 0.01\n",
        "\n",
        "# At what time points you want to save the solution\n",
        "saveat = diffrax.SaveAt(ts=jnp.linspace(t0,t1,Nt))\n",
        "\n",
        "def solve_transport_equation(y0,S,sig,args):\n",
        "  args['S']    = S\n",
        "  args['sig'] = sig\n",
        "  sol = diffrax.diffeqsolve(transport_equation_term, solver, y0=y0, args = args,\n",
        "                        t0=t0, t1=t1, dt0=dt0, max_steps=1000000,\n",
        "                        saveat=saveat, stepsize_controller=stepsize_controller)\n",
        "\n",
        "  fs = sol.ys.reshape((-1,args['Nm'],args['Nx']))\n",
        "  phis = jnp.sum(args['w_m'][None,:,None]*fs[:,:,:],axis=1)\n",
        "  return phis"
      ],
      "metadata": {
        "id": "eB9YZTLre6qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike for the ODE, we must also define the grid within the ($x,\\mu$) domain on which we will numerically solve the transport equation:"
      ],
      "metadata": {
        "id": "h0GsJzY7JD_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Nm = 20\n",
        "mu_edges = jnp.linspace(-1,1,Nm+1)\n",
        "mu_m = (mu_edges[1:]+mu_edges[:-1])/2\n",
        "w_m  = mu_edges[1:]-mu_edges[:-1]\n",
        "\n",
        "Nx = 400\n",
        "dx = 0.1\n",
        "\n",
        "y0 = jnp.zeros(Nm*Nx)\n",
        "\n",
        "args = {'Nm' : Nm, 'Nx' : Nx, 'dx' : dx,\n",
        "        'mu_m' : mu_m, 'w_m' : w_m}"
      ],
      "metadata": {
        "id": "ku5kPWh4JEUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define a source of particles:\n",
        "\n",
        "$$\n",
        "S(x,t) = \\Theta(t_{on}-t)\\left\\{\\exp\\left[-\\left(\\frac{x-\\mu_1}{\\delta}\\right)^2\\right]+\\exp\\left[-\\left(\\frac{x-\\mu_2}{\\delta}\\right)^2\\right]\\right\\}\n",
        "$$\n",
        "with $t_{on} = 5$, $\\mu_{1} = 15$, $\\mu_{2} = 25$, and $\\delta = 4$, and $\\Theta$ is the Heaviside function.\n",
        "\n",
        "This is evaluated on the spatial grid and a closure is created such that we have a function of time only for the method of lines."
      ],
      "metadata": {
        "id": "n-kJMVZKJFQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def double_gaussian_source(t,dx,Nx):\n",
        "  # To be completed\n",
        "  # Implement the source function defined above\n",
        "  x = jnp.arange(Nx)*dx\n",
        "  S =\n",
        "  return S\n",
        "\n",
        "S = lambda t : double_gaussian_source(t,dx,Nx)"
      ],
      "metadata": {
        "id": "ISkx2QPbhVPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now solve the transport equation for various $\\sigma$ values and inspect the solutions."
      ],
      "metadata": {
        "id": "eo7iWYXTa_Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cs = ['r','g','b']\n",
        "sigs = [0.01,0.1,1.0]\n",
        "\n",
        "for c,sig in zip(cs,sigs):\n",
        "\n",
        "  phis = solve_transport_equation(y0,S,sig,args)\n",
        "\n",
        "  labels = [f'$\\sigma$ = {sig}'] + [None] * (phis.shape[0] - 2)\n",
        "  plt.plot(phis[1:,:].T,c=c,alpha=0.5,label=labels)\n",
        "\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "lskyFkUUYg8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "1) What is the purpose of the $w_m$ array?\n",
        "\n",
        "2) Is it clear why solutions with larger $\\sigma$ values spread out more slowly?\n",
        "\n",
        "3) For $\\sigma = 0$ and late times, in which region will $f(\\mu > 0) \\gg f(\\mu < 0)$?"
      ],
      "metadata": {
        "id": "mxQ42_zgE2yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2\n",
        "\n",
        "The transport equation above is defined in three dimensions: 1 space, 1 direction and time. It is often the case that multi-dimensional problems are more computationally expensive or present unique numerical challenges. Therefore, reduced models seek to retain some of the dynamics of the full high dimensional model but within reduced dimensions.\n",
        "\n",
        "For our example, we might be interested in a PDE for the evolution of $\\phi(x,t)$ only. By integrating out the $\\mu$ dimension of the transport equation, we arrive at the following:\n",
        "\n",
        "$$ \\frac{\\partial \\phi}{\\partial t} = -\\frac{\\partial F_\\phi}{\\partial x} + S $$\n",
        "\n",
        "Where the flux of $\\phi$ is:\n",
        "\n",
        "$$ F_{\\phi} = \\int \\mu f(x,\\mu,t) d\\mu $$\n",
        "\n",
        "Diffusion theory suggests a Fick's law for this flux of the form:\n",
        "\n",
        "$$ F_{\\phi} = - D \\frac{\\partial \\phi}{\\partial x} $$\n",
        "\n",
        "Leading to a (potentially non-linear) heat equation:\n",
        "\n",
        "$$ \\frac{\\partial \\phi}{\\partial t} = \\frac{\\partial }{\\partial x}\\left( D \\frac{\\partial \\phi}{\\partial x} \\right) + S $$\n",
        "\n",
        "Question is, what is $D$? This can be stated as an optimisation problem, for what $D$ is the diffusion solution most similar to the transport solution.\n",
        "\n",
        "In this part of the exercise we will use optax to train a differentiable simulator\n",
        "\n",
        "***\n",
        "\n",
        "As means as of an introduction to optax, we will solve a very simple optimisation problem:\n",
        "\n",
        "$$\n",
        "\\mathrm{argmin}_p f(p) = \\underline{p}^T \\cdot \\underline{\\underline{d}} \\cdot \\underline{p}\n",
        "$$\n",
        "\n",
        "which is trivially solvable with $\\underline{p} = \\underline{0}$. We will also use this oppurtunity to make use of JAX's random number generators.\n",
        "\n",
        "Below is some code to set up the problem for randomly generated $\\underline{\\underline{d}}$ and starting location for the optimisation problem $\\underline{p}_0$."
      ],
      "metadata": {
        "id": "Xe0rEZ3EkZ4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNG initialisation\n",
        "key = jax.random.key(0)\n",
        "\n",
        "def example_loss(p,d):\n",
        "  return jnp.dot(p.T,jnp.dot(d,p))\n",
        "\n",
        "# Dimension of input space\n",
        "Np = 10\n",
        "# Convex shape of input space\n",
        "d = jax.random.normal(key,shape=(Np,Np))\n",
        "# Random positive semi-definite\n",
        "d = jnp.dot(d,d.T)\n",
        "# Random starting location\n",
        "key, subkey = jax.random.split(key)\n",
        "p0 = jax.random.normal(subkey,shape=(Np,))\n",
        "print(f'Starting loss: {example_loss(p0,d)}')\n"
      ],
      "metadata": {
        "id": "h_gkg-rckblS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optax defines the optimisation workflow through two important components:\n",
        "\n",
        "1. The optimizer: this defines the optimizer algorithm and uses the gradients of the loss (combined with the optimizer hyperparameters) to update the trainable parameters\n",
        "2. The optimizer state: this defines the trainable parameters\n",
        "\n",
        "In the code below, we define these optax components for our simple convex optimisation problem. We also define the gradient of the loss via AD. Finally, we introduce a training loop which iteratively updates the parameters via the adam optimiser."
      ],
      "metadata": {
        "id": "N7o4Yl7qm4YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters of the model + optimizer.\n",
        "learning_rate = 1e-1\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(p0)\n",
        "\n",
        "# A simple update loop\n",
        "Nepoch = 200\n",
        "grad_loss = jax.value_and_grad(example_loss)\n",
        "p = p0.copy()\n",
        "history = []\n",
        "for _ in range(Nepoch):\n",
        "  loss,grads = grad_loss(p, d)\n",
        "  # Optax optimizer uses the gradients to update the parameters and the optimiser state\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  p = optax.apply_updates(p, updates)\n",
        "  history.append(loss)"
      ],
      "metadata": {
        "id": "o60-KSE9lTyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the loss history, we see it converging towards the global optimum at 0."
      ],
      "metadata": {
        "id": "QwxAjoivm2GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogy(history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "print(f'Final p mag: {jnp.linalg.norm(p)}')"
      ],
      "metadata": {
        "id": "VsXQ9YYymRst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this simple example of how to use optax, we can now apply it to optimising the numerical solution to PDEs.\n",
        "\n",
        "We must now follow the same exercise of defining our numerical solution to the heat equation. The method of lines finite differencing for the heat equation is as follows:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\phi_i}{\\partial t} = D\\frac{\\phi_{i-1}-2\\phi_{i}+\\phi_{i+1}}{\\Delta x^2} + S\n",
        "$$\n",
        "\n",
        "We will reuse the same solver and stepsize_controller as for the transport equation.\n",
        "\n",
        "There are a few lines to be completed in the following code."
      ],
      "metadata": {
        "id": "ruxtBvsjnMsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def heat_equation_dydt(t,y,args):\n",
        "  Nx = args['Nx']\n",
        "  D, dx = args['D'], args['dx']\n",
        "  # Compute finite differences\n",
        "  # Use the ghost cell technique again\n",
        "  y_ghost = jnp.concatenate([y[:1],y,y[-1:]],axis=0)\n",
        "  # To be completed\n",
        "  # See the example used in the transport equation example\n",
        "  dydt =\n",
        "  # Add isotropic source term\n",
        "  dydt = dydt+args['S'](t)\n",
        "  return dydt\n",
        "\n",
        "# We convert our python JAX function to a diffrax ODETerm\n",
        "heat_equation_term = diffrax.ODETerm(heat_equation_dydt)\n",
        "\n",
        "def solve_heat_equation(y0,D,args):\n",
        "  args['D'] = D\n",
        "  sol = diffrax.diffeqsolve(heat_equation_term, solver, y0=y0, args = args,\n",
        "                        t0=t0, t1=t1, dt0=dt0, max_steps=1000000,\n",
        "                        saveat=saveat, stepsize_controller=stepsize_controller)\n",
        "\n",
        "  phis = sol.ys\n",
        "  return phis"
      ],
      "metadata": {
        "id": "I_7g0RZji7RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to find the optimal value of a scalar diffusivity, $D$, we must define a loss function. In this case, we will use the MSE between the $\\phi$s calculated by transport and the diffusion approximation evaluated at the SaveAt times:\n",
        "\n",
        "$$\n",
        "L \\propto \\sum_{t \\in \\mathrm{SaveAt}} \\sum_i (\\phi_{D,i}(t)-\\phi_{i}(t))^2\n",
        "$$"
      ],
      "metadata": {
        "id": "SppRVjHiijKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diffusion_to_transport_loss(D,phis_transport,args):\n",
        "  y_heat_0 = jnp.zeros(args['Nx'])\n",
        "  phis_heat = solve_heat_equation(y_heat_0,D[0],args)\n",
        "  # Mean Squared Error\n",
        "  return jnp.mean((phis_transport-phis_heat)**2)\n",
        "\n",
        "grad_PDE_loss = jax.value_and_grad(diffusion_to_transport_loss)"
      ],
      "metadata": {
        "id": "xTEov3eNoe5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the pieces needed to solve the optimisation problem we have defined in this section.\n",
        "\n",
        "***\n",
        "\n",
        "To begin optimising our diffusion PDE solution we must create the training data and initialise the optimiser:"
      ],
      "metadata": {
        "id": "x388bACzij29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "sig = 1.0\n",
        "phis_transport = solve_transport_equation(y0,S,sig,args)\n",
        "\n",
        "# Initialize parameters\n",
        "D_opt = jnp.array([1.0])\n",
        "diffusion_args = {'Nx' : Nx, 'dx' : dx, 'S' : S}\n",
        "\n",
        "# Initialize optimizer\n",
        "learning_rate = 1e-2\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(D_opt)"
      ],
      "metadata": {
        "id": "xLia3u6VkeIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we can set up a very simple training loop analogous to the first simple example we gave of how to use optax.\n",
        "\n",
        "Copy over the previous example and update it to call the grad_PDE_loss function to obtain loss and grads"
      ],
      "metadata": {
        "id": "aC65G-TpikxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy over and modify the simple optax update loop from above\n",
        "\n",
        "# After training\n",
        "plt.semilogy(history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "print(f'Final D: {D_opt}')"
      ],
      "metadata": {
        "id": "Ykv-Z3_nqRMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our optimal value of $D$, we can visual compare the output of our trained diffusion simulator and the transport solution:"
      ],
      "metadata": {
        "id": "FysQw7HSoA5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(phis_transport.T,alpha=0.5,c='r')\n",
        "y_heat_0 = jnp.zeros(args['Nx'])\n",
        "phis_heat = solve_heat_equation(y_heat_0,D_opt[0],diffusion_args)\n",
        "plt.plot(phis_heat.T,ls='--',c='k')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel(r\"$\\phi$\")"
      ],
      "metadata": {
        "id": "2Ds4D_K0rCLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the optimal $D$ value for different $\\sigma$ values, we can package up the previous initilisation and optimisation code into a function of $\\sigma$ only."
      ],
      "metadata": {
        "id": "emQX9vdwipg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimal_D_for_sig(sig,Nepoch=100,learning_rate=1e-2):\n",
        "  phis_transport = solve_transport_equation(y0,S,sig,args)\n",
        "\n",
        "  # Initialize parameters\n",
        "  # To be completed\n",
        "  D_opt =\n",
        "  diffusion_args =\n",
        "\n",
        "  # Initialize optimizer\n",
        "  # To be completed\n",
        "  optimizer =\n",
        "  opt_state =\n",
        "\n",
        "  # A simple update loop\n",
        "  # To be completed\n",
        "\n",
        "  return D_opt[0],loss"
      ],
      "metadata": {
        "id": "QWrSK5TcsXeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigs = jnp.array([100.0,10.0,1.0,0.1,0.01])\n",
        "D_opts = []\n",
        "losses = []\n",
        "for sig in sigs:\n",
        "  D_opt,loss = optimal_D_for_sig(sig)\n",
        "  D_opts.append(D_opt)\n",
        "  losses.append(loss)"
      ],
      "metadata": {
        "id": "JoGEKQJYmhjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the results of optimal $D$ coefficient as a function of $\\sigma$ and also inspect the obtained loss."
      ],
      "metadata": {
        "id": "NB1OD8zamjqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add plotting script\n"
      ],
      "metadata": {
        "id": "phKXWNggmeVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "1) Is the diffusion approximation more appropriate for large or small $\\sigma$, i.e. for which is a small loss obtainable?\n",
        "\n",
        "2) Why might one expect $D$ to be a decreasing function of $\\sigma$?\n",
        "\n",
        "3) Can you think of a non-zero source term $S(x,t)$ which would make transport and diffusion solutions equal for any $D$ value?\n"
      ],
      "metadata": {
        "id": "mOgfSMLtExL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Advanced) Neural Differential Equations\n",
        "\n",
        "In the most simple form, a neural differential equation (NDE) has the following structure:\n",
        "\n",
        "$$\n",
        "\\frac{d y}{d t} = \\mathcal{N}_\\theta(y,t)\n",
        "$$\n",
        "\n",
        "where $\\mathcal{N}_\\theta(y,t)$ is a neural network with parameters $\\theta$.\n",
        "\n",
        "The numerical solution can be computed using traditional techniques used for ODEs and PDEs. The adjoint solution can then be used to train the neural network to improve the solution. Due to the flexibility of neural networks, NDEs can describe a wide variety of systems in a time-continuous manner.\n",
        "\n",
        "In the following we will consider a simple example where we will use a NDE to describe the response of an electrical circuit for which we have collected data but do not know its internal structure.\n",
        "\n",
        "For neural networks in JAX we will use equinox:\n",
        "\n",
        "- equinox [documentation](https://docs.kidger.site/equinox/)\n",
        "\n",
        "and then diffrax and optax can be used in the same way as before to solve both the forward and adjoint problems."
      ],
      "metadata": {
        "id": "QMzJ6LnnrObF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install equinox\n",
        "import equinox as eqx\n",
        "import jax.nn as jnn\n",
        "import jax.random as jrandom"
      ],
      "metadata": {
        "id": "ROV_o5tgvK42",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the theory of electical circuits, we propose a simple NDE of the form:\n",
        "\n",
        "$$ \\frac{d}{dt} \\begin{bmatrix}I \\\\ h\\end{bmatrix} = \\begin{bmatrix}h \\\\ \\mathcal{N}_\\theta(I,h)\\end{bmatrix} + \\begin{bmatrix}0 \\\\ \\frac{dV_{ext.}}{dt}\\end{bmatrix}$$\n",
        "\n",
        "Where $I$ is the electical current and $V_{ext.}$ is the externally applied voltage. We introduce a hidden state $h$ which linearly responds to the temporal gradient of the applied voltage. The neural network then computes the temporal response of the hidden state given the solution values at time $t$.\n",
        "\n",
        "First, we load in the training data, a data set of ($t$, $I$) for a oscillating applied voltage $V(t,\\omega,T)$ at various $\\omega$ values. The currents ($I$) have been corrupted by noise and therefore our NDE model must be robust to this noise to be useful."
      ],
      "metadata": {
        "id": "46UbioTOvRxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = 10.0 #s for all data sets\n",
        "\n",
        "# Upload the data provided in course git repo to the sample_data folder\n",
        "# <- Open the colab file browser by the folder icon of the left\n",
        "omegas = jnp.load('./sample_data/omegas.npy')\n",
        "ts = jnp.load('./sample_data/ts.npy')\n",
        "Is = jnp.load('./sample_data/Is.npy')\n",
        "\n",
        "# Save only one data set for testing in this simple example\n",
        "ts_train, Is_train, omegas_train = ts[:], Is[:-1,:], omegas[:-1]\n",
        "ts_test, Is_test, omegas_test    = ts[:], Is[-1:,:], omegas[-1:]\n",
        "\n",
        "# Applied voltage function\n",
        "def V(t,omega,T):\n",
        "    \"\"\"\n",
        "    Applied voltage function, frequency increases quadratically in time\n",
        "    \"\"\"\n",
        "    omega0 = omega*(2*t/T)\n",
        "    return jnp.sin(omega0*t)\n",
        "\n",
        "# To be completed\n",
        "# Create a gradient function of V, dVdt\n",
        "# Also create a vmapped version, which maps over the time dimension (see in_axes argument of jax.vmap)\n",
        "dVdt =\n",
        "dVdt_vmapped ="
      ],
      "metadata": {
        "id": "jDBzn7__yc5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets quickly view the data and the forcing term (dV/dt):"
      ],
      "metadata": {
        "id": "55Xs2e8_741O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(dpi=100)\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax2 = fig.add_subplot(212,sharex=ax1)\n",
        "ax1.plot(ts,Is.T,'k')\n",
        "for omega in omegas:\n",
        "  ax2.plot(ts,dVdt_vmapped(ts,omega,T),'r')\n",
        "ax1.set_xlim(ts[0],ts[-1])\n",
        "ax2.set_xlabel('t')\n",
        "ax1.set_ylabel('I')\n",
        "ax2.set_ylabel('dVdt')"
      ],
      "metadata": {
        "id": "9nOpVy5_6iJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up an equinox Module that contains an MultiLayer Perceptron (MLP) for $\\mathcal{N}_\\theta(I,h)$ and its call method evaluated the right hand side of the full NDE."
      ],
      "metadata": {
        "id": "UYLPAozfydZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NDE solution\n",
        "# Following https://docs.kidger.site/diffrax/examples/neural_ode/\n",
        "class NeuralODE_RHS(eqx.Module):\n",
        "    mlp: eqx.nn.MLP\n",
        "\n",
        "    def __init__(self, in_size, out_size, width_size, depth, *, key, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mlp = eqx.nn.MLP(\n",
        "            in_size=in_size,\n",
        "            out_size=out_size,\n",
        "            width_size=width_size,\n",
        "            depth=depth,\n",
        "            activation=jnn.leaky_relu,\n",
        "            key=key,\n",
        "            use_bias=False,\n",
        "            use_final_bias=False\n",
        "        )\n",
        "\n",
        "    def __call__(self, t, y, args):\n",
        "        # To be completed\n",
        "        dhdt =\n",
        "        dIdt =\n",
        "        # Stack the temporal responses into dydt\n",
        "        dydt =\n",
        "        return dydt\n",
        "\n",
        "class NeuralODE(eqx.Module):\n",
        "    in_size: int\n",
        "    func: NeuralODE_RHS\n",
        "\n",
        "    def __init__(self, in_size, out_size, width_size, depth, *, key, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.in_size = in_size\n",
        "        self.func = NeuralODE_RHS(in_size, out_size, width_size, depth, key=key)\n",
        "\n",
        "    def __call__(self, ts, omega, args):\n",
        "        \"\"\"\n",
        "        Similar to our examples above, we set up diffeqsolve\n",
        "        but now our ODETerm uses our NeuralODE_RHS equinox Module.\n",
        "        \"\"\"\n",
        "        # Add relevant data to the args dictionary\n",
        "        args['omega'] = omega\n",
        "        solution = diffrax.diffeqsolve(\n",
        "            diffrax.ODETerm(self.func),\n",
        "            diffrax.Heun(),\n",
        "            t0=ts[0],\n",
        "            t1=ts[-1],\n",
        "            dt0=ts[1] - ts[0],\n",
        "            y0=jnp.zeros(self.in_size),\n",
        "            args=args,\n",
        "            stepsize_controller=diffrax.PIDController(rtol=1e-2, atol=1e-4),\n",
        "            saveat=diffrax.SaveAt(ts=ts),\n",
        "            max_steps=int(1e6)\n",
        "        )\n",
        "        return solution.ys"
      ],
      "metadata": {
        "id": "t8HUUDdprvj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will train our NDE using the adjoint method and the available training data of ($t$,$I$,$\\omega$).\n",
        "\n",
        "The training process will look very similar to what you implemented above in the diffusion example. The only thing to notice is that break the learning up over increasingly long sections of the data. So we train on the first x time steps for n epochs and then the next y time steps for m epochs, etc.. This is a standard trick to avoid getting caught in a local minimum.\n",
        "\n",
        "There are a few lines to complete in order to run the training of the NDE."
      ],
      "metadata": {
        "id": "7nH31C6o0-6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_NDE(\n",
        "    ts,ys,omegas,args,\n",
        "    width_size,depth,\n",
        "    optimiser,\n",
        "    lr_strategy,\n",
        "    steps_strategy,\n",
        "    length_strategy,\n",
        "    seed=420,\n",
        "    print_every=50\n",
        "):\n",
        "    \"\"\"\n",
        "    ts : jax array of data times\n",
        "    ys : jax array of data currents\n",
        "    omegas : jax array of data drive frequencies\n",
        "    width_size : number of neurons in hidden layers\n",
        "    depth : number of hidden layers\n",
        "    optimiser : optax optimiser to be used\n",
        "    lr_strategy : learning rate schedule in tuple\n",
        "    steps_strategy : number of training steps in tuple\n",
        "    length_strategy : fraction of training data used in training in tuple\n",
        "    seed : PRNG seed value\n",
        "    print_every : print every n steps of training\n",
        "    \"\"\"\n",
        "    key = jrandom.PRNGKey(seed)\n",
        "    __, model_key = jrandom.split(key)\n",
        "\n",
        "    length_size = ts.shape[0]\n",
        "\n",
        "    # To be completed, what is the shape of our input and output layer\n",
        "    in_size =\n",
        "    out_size =\n",
        "\n",
        "    model = NeuralODE(in_size, out_size, width_size, depth, key=model_key)\n",
        "\n",
        "    @eqx.filter_value_and_grad\n",
        "    def grad_loss(model, ti, yi, omegas):\n",
        "        # Handle batch\n",
        "        batched_model = jax.vmap(model,in_axes=(None,0,None))\n",
        "        y_pred = batched_model(ti, omegas, args)\n",
        "        # Only compute the MSE using the NDE current (I) prediction\n",
        "        # To be completed\n",
        "        MSE =\n",
        "        return MSE\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def make_step(ti, yi, omegas, model, opt_state):\n",
        "        loss, grads = grad_loss(model, ti, yi, omegas)\n",
        "        updates, opt_state = optim.update(grads, opt_state)\n",
        "        model = eqx.apply_updates(model, updates)\n",
        "        return loss, model, opt_state\n",
        "\n",
        "    count = 0\n",
        "    for lr, steps, length in zip(lr_strategy, steps_strategy, length_strategy):\n",
        "        optim = optimiser(lr)\n",
        "        opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "        _ts = ts[: int(length_size * length)]\n",
        "        _ys = ys[:, : int(length_size * length)]\n",
        "        for step in range(steps):\n",
        "            count += 1\n",
        "            start = time.time()\n",
        "            loss, model, opt_state = make_step(_ts, _ys, omegas, model, opt_state)\n",
        "            end = time.time()\n",
        "            if (step % print_every) == 0 or step == steps - 1:\n",
        "                print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "FZntHwF9zt3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will make a very small neural network model in this case, both for quick training and for regularisation such that we do not overfit the noisy data."
      ],
      "metadata": {
        "id": "_f96niUdKwpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP hyperparameters\n",
        "width_size = 4  # Number of neurons in hidden layers\n",
        "depth = 2       # Number of hidden layers (including output layer)\n",
        "\n",
        "optimiser = optax.adabelief\n",
        "\n",
        "NDE_args = {'dVdt' : dVdt, 'T' : T}\n",
        "\n",
        "# Training should take just a 1-2 mins\n",
        "trained_model = train_NDE(ts_train,Is_train,omegas_train,\n",
        "                          NDE_args,\n",
        "                          width_size,depth,\n",
        "                          optimiser,\n",
        "                          lr_strategy=(1e-2,1e-2,1e-3),\n",
        "                          steps_strategy=(200,200,400),\n",
        "                          length_strategy=(0.5,1.0,1.0))"
      ],
      "metadata": {
        "id": "idCQcOf1-eBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have trained our NDE we can test its results at an unseen value of $\\omega$.\n",
        "\n",
        "Below, write code to compute the NDE solution at all omegas (including the test data) and compare the predicted and true current values."
      ],
      "metadata": {
        "id": "vglsERoj1kXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only omega changes between data samples\n",
        "compute_trained_model_pred = lambda omega : trained_model(ts, omega, NDE_args)\n",
        "\n",
        "# Plot trained model against train and test data"
      ],
      "metadata": {
        "id": "bQudRVjez4mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the tuned hyperparameters above, this NDE can capture the behaviour of the electrical circuit with high accuracy on both training and test data.\n",
        "\n",
        "Including neural networks in differential equation methods opens up a wide range of possibilities. These include Physics-Informed Neural Networks (PINNs) and Neural Operators to name a few."
      ],
      "metadata": {
        "id": "1wclo1eW27jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key takeaways\n",
        "\n",
        "- Differentiable simulators require the following components:\n",
        "  1. A pre-defined ODE or PDE structure\n",
        "  2. A number of trainable parameters which are not a-priori known\n",
        "  3. A numerical scheme for solution (finite differencing, time-stepping, etc.)\n",
        "  4. A loss function which defines an optimal model\n",
        "  5. A means to minimise the loss (adjoint solver, optimiser, etc.)\n",
        "- Diffrax is a JAX library for the numerical solution (both forward and adjoint) of differential equations\n",
        "- Optax is a JAX library for optimisation which can be easily interfaced with diffrax\n",
        "- (Advanced) One can construct differential equations which include neural networks. Equinox is a JAX library which can be used for this task."
      ],
      "metadata": {
        "id": "VLsuVGWjy6It"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uR5y3pMA2CjK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}